#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë³‘ë ¬ ì²˜ë¦¬ ê¸°ë°˜ 3ë…„ì¹˜ ì‹¤ì œ CMEMS + í•´ì–‘ìƒë¬¼ ë°ì´í„° í†µí•© AI í•™ìŠµ ì‹œìŠ¤í…œ
ë‚ ì§œë³„ ì¤‘ë³µ ë°©ì§€ ë° íš¨ìœ¨ì  ë³‘ë ¬ ë°ì´í„° ìˆ˜ì§‘
"""

import os
import sys
import pandas as pd
import numpy as np
import logging
import json
from datetime import datetime, timedelta
from multiprocessing import Pool, Manager, Lock, Value, Queue
from concurrent.futures import ProcessPoolExecutor, as_completed
import threading
import time
import pickle
from pathlib import Path

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('parallel_training.log', encoding='utf-8')
    ]
)

def log(message):
    logging.info(message)

# ê¸°ì¡´ í•¨ìˆ˜ë“¤ ì„í¬íŠ¸
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from real_data_system import MarineRealDataCollector

# CMEMS ë°ì´í„° ê´€ë ¨ í•¨ìˆ˜ë“¤ì„ ê¸°ì¡´ íŒŒì¼ì—ì„œ ê°€ì ¸ì˜´
from three_year_cmems_trainer import (
    download_with_lock,
    download_cmems_data_for_date,
    collect_cmems_data_for_date
)

class ParallelThreeYearTrainer:
    """ë³‘ë ¬ ì²˜ë¦¬ 3ë…„ì¹˜ CMEMS + í•´ì–‘ìƒë¬¼ ë°ì´í„° í†µí•© í•™ìŠµ ì‹œìŠ¤í…œ"""
    
    def __init__(self, num_processes=4):
        """
        Args:
            num_processes: ë³‘ë ¬ í”„ë¡œì„¸ìŠ¤ ê°œìˆ˜ (ê¸°ë³¸ê°’: 4)
        """
        self.num_processes = num_processes
        self.data_collector = MarineRealDataCollector()
        
        # í•œêµ­ ì—°ì•ˆ ì£¼ìš” ê²©ìì 
        self.grid_points = [
            # ë™í•´
            (37.5, 129.0), (37.0, 129.5), (36.5, 130.0), (36.0, 130.5),
            (35.5, 129.5), (35.0, 129.0), (34.5, 129.5), (34.0, 130.0),
            
            # ë‚¨í•´
            (34.0, 128.5), (34.5, 128.0), (35.0, 127.5), (35.5, 127.0),
            (34.0, 127.0), (34.5, 126.5), (35.0, 126.0), (35.5, 125.5),
            
            # ì„œí•´
            (37.0, 126.0), (36.5, 126.5), (36.0, 127.0), (35.5, 126.0),
            (35.0, 125.5), (34.5, 125.0), (34.0, 124.5), (33.5, 125.0)
        ]
        
        # ë‚ ì§œ ë²”ìœ„ ì„¤ì • (CMEMS ë°ì´í„° ê°€ìš© ë²”ìœ„)
        self.start_date = datetime(2022, 6, 1)
        self.end_date = datetime(2024, 9, 13)
        
        # ì£¼ ë‹¨ìœ„ ë‚ ì§œ ëª©ë¡ ìƒì„± (120ê°œ)
        self.training_dates = []
        current_date = self.start_date
        while current_date <= self.end_date:
            self.training_dates.append(current_date.strftime('%Y-%m-%d'))
            current_date += timedelta(days=7)  # ì£¼ ë‹¨ìœ„
        
        # ì§„í–‰ ìƒíƒœ ê´€ë¦¬
        self.progress_file = "parallel_training_progress.json"
        self.completed_dates = set()
        self.data_dir = "parallel_training_data"
        os.makedirs(self.data_dir, exist_ok=True)
        
        log(f"ë³‘ë ¬ í›ˆë ¨ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        log(f"í”„ë¡œì„¸ìŠ¤ ìˆ˜: {self.num_processes}")
        log(f"ê²©ìì : {len(self.grid_points)}ê°œ")
        log(f"í›ˆë ¨ ë‚ ì§œ: {len(self.training_dates)}ì¼ ({self.start_date.strftime('%Y-%m-%d')} ~ {self.end_date.strftime('%Y-%m-%d')})")
        
        self.load_progress()

    def load_progress(self):
        """ì§„í–‰ ìƒíƒœ ë¡œë“œ"""
        try:
            if os.path.exists(self.progress_file):
                with open(self.progress_file, 'r', encoding='utf-8') as f:
                    progress_data = json.load(f)
                    self.completed_dates = set(progress_data.get('completed_dates', []))
                log(f"ì§„í–‰ ìƒíƒœ ë¡œë“œ: {len(self.completed_dates)}ì¼ ì™„ë£Œ")
            else:
                log("ìƒˆë¡œìš´ í›ˆë ¨ ì„¸ì…˜ ì‹œì‘")
        except Exception as e:
            log(f"ì§„í–‰ ìƒíƒœ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.completed_dates = set()

    def save_progress(self):
        """ì§„í–‰ ìƒíƒœ ì €ì¥"""
        try:
            progress_data = {
                'completed_dates': list(self.completed_dates),
                'last_updated': datetime.now().isoformat()
            }
            with open(self.progress_file, 'w', encoding='utf-8') as f:
                json.dump(progress_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            log(f"ì§„í–‰ ìƒíƒœ ì €ì¥ ì‹¤íŒ¨: {e}")

    def get_pending_dates(self):
        """ì•„ì§ ì™„ë£Œë˜ì§€ ì•Šì€ ë‚ ì§œ ëª©ë¡ ë°˜í™˜"""
        return [date for date in self.training_dates if date not in self.completed_dates]

def process_single_date(date_str, grid_points, data_dir):
    """ë‹¨ì¼ ë‚ ì§œì— ëŒ€í•œ ë°ì´í„° ìˆ˜ì§‘ ë° ì €ì¥ (ë³‘ë ¬ í”„ë¡œì„¸ìŠ¤ì—ì„œ ì‹¤í–‰)"""
    try:
        process_id = os.getpid()
        log(f"[PID:{process_id}] {date_str} ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
        
        # íŒŒì¼ ê²½ë¡œ ì„¤ì •
        date_file = os.path.join(data_dir, f"data_{date_str.replace('-', '_')}.csv")
        
        # ì´ë¯¸ ì™„ë£Œëœ íŒŒì¼ì´ ìˆìœ¼ë©´ ê±´ë„ˆë›°ê¸°
        if os.path.exists(date_file):
            log(f"[PID:{process_id}] {date_str} ì´ë¯¸ ì™„ë£Œë¨ - ê±´ë„ˆë›°ê¸°")
            return date_str
        
        # í•´ì–‘ìƒë¬¼ ë°ì´í„° ìˆ˜ì§‘
        data_collector = MarineRealDataCollector()
        bio_df = data_collector.collect_daily_training_data(date_str, grid_points)
        
        if bio_df.empty:
            log(f"[PID:{process_id}] {date_str} í•´ì–‘ìƒë¬¼ ë°ì´í„° ì—†ìŒ")
            # ë¹ˆ ë°ì´í„°í”„ë ˆì„ì´ë¼ë„ íŒŒì¼ ìƒì„± (ì™„ë£Œ í‘œì‹œìš©)
            bio_df = pd.DataFrame({
                'lat': [point[0] for point in grid_points],
                'lon': [point[1] for point in grid_points]
            })
            for species in data_collector.target_species:
                bio_df[species] = 0
        
        # CMEMS í™˜ê²½ ë°ì´í„° ìˆ˜ì§‘
        cmems_df = collect_cmems_data_for_date(date_str, grid_points)
        
        # ë°ì´í„° í†µí•©
        if not cmems_df.empty:
            # lat, lonìœ¼ë¡œ ë³‘í•©
            integrated_df = pd.merge(bio_df, cmems_df, on=['lat', 'lon'], how='outer')
        else:
            integrated_df = bio_df.copy()
            # CMEMS ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ìœ¼ë¡œ ì±„ì›€
            env_cols = ['sea_water_temperature', 'sea_water_salinity', 'sea_surface_height', 
                       'mixed_layer_depth', 'dissolved_oxygen', 'net_primary_productivity']
            for col in env_cols:
                integrated_df[col] = 0.0
        
        # ë‚ ì§œ ì»¬ëŸ¼ ì¶”ê°€
        integrated_df['date'] = date_str
        
        # ê²°ê³¼ ì €ì¥
        integrated_df.to_csv(date_file, index=False, encoding='utf-8')
        
        log(f"[PID:{process_id}] {date_str} ì™„ë£Œ: {len(integrated_df)}í–‰ Ã— {len(integrated_df.columns)}ì—´")
        
        return date_str
        
    except Exception as e:
        log(f"[PID:{process_id}] {date_str} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return None

class ParallelThreeYearTrainer:
    """ë³‘ë ¬ ì²˜ë¦¬ 3ë…„ì¹˜ CMEMS + í•´ì–‘ìƒë¬¼ ë°ì´í„° í†µí•© í•™ìŠµ ì‹œìŠ¤í…œ"""
    
    def __init__(self, num_processes=4):
        """
        Args:
            num_processes: ë³‘ë ¬ í”„ë¡œì„¸ìŠ¤ ê°œìˆ˜ (ê¸°ë³¸ê°’: 4)
        """
        self.num_processes = num_processes
        self.data_collector = MarineRealDataCollector()
        
        # í•œêµ­ ì—°ì•ˆ ì£¼ìš” ê²©ìì 
        self.grid_points = [
            # ë™í•´
            (37.5, 129.0), (37.0, 129.5), (36.5, 130.0), (36.0, 130.5),
            (35.5, 129.5), (35.0, 129.0), (34.5, 129.5), (34.0, 130.0),
            
            # ë‚¨í•´
            (34.0, 128.5), (34.5, 128.0), (35.0, 127.5), (35.5, 127.0),
            (34.0, 127.0), (34.5, 126.5), (35.0, 126.0), (35.5, 125.5),
            
            # ì„œí•´
            (37.0, 126.0), (36.5, 126.5), (36.0, 127.0), (35.5, 126.0),
            (35.0, 125.5), (34.5, 125.0), (34.0, 124.5), (33.5, 125.0)
        ]
        
        # ë‚ ì§œ ë²”ìœ„ ì„¤ì • (CMEMS ë°ì´í„° ê°€ìš© ë²”ìœ„)
        self.start_date = datetime(2022, 6, 1)
        self.end_date = datetime(2024, 9, 13)
        
        # ì£¼ ë‹¨ìœ„ ë‚ ì§œ ëª©ë¡ ìƒì„± (120ê°œ)
        self.training_dates = []
        current_date = self.start_date
        while current_date <= self.end_date:
            self.training_dates.append(current_date.strftime('%Y-%m-%d'))
            current_date += timedelta(days=7)  # ì£¼ ë‹¨ìœ„
        
        # ì§„í–‰ ìƒíƒœ ê´€ë¦¬
        self.progress_file = "parallel_training_progress.json"
        self.completed_dates = set()
        self.data_dir = "parallel_training_data"
        os.makedirs(self.data_dir, exist_ok=True)
        
        log(f"ë³‘ë ¬ í›ˆë ¨ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        log(f"í”„ë¡œì„¸ìŠ¤ ìˆ˜: {self.num_processes}")
        log(f"ê²©ìì : {len(self.grid_points)}ê°œ")
        log(f"í›ˆë ¨ ë‚ ì§œ: {len(self.training_dates)}ì¼ ({self.start_date.strftime('%Y-%m-%d')} ~ {self.end_date.strftime('%Y-%m-%d')})")
        
        self.load_progress()

    def load_progress(self):
        """ì§„í–‰ ìƒíƒœ ë¡œë“œ"""
        try:
            if os.path.exists(self.progress_file):
                with open(self.progress_file, 'r', encoding='utf-8') as f:
                    progress_data = json.load(f)
                    self.completed_dates = set(progress_data.get('completed_dates', []))
                log(f"ì§„í–‰ ìƒíƒœ ë¡œë“œ: {len(self.completed_dates)}ì¼ ì™„ë£Œ")
            else:
                log("ìƒˆë¡œìš´ í›ˆë ¨ ì„¸ì…˜ ì‹œì‘")
        except Exception as e:
            log(f"ì§„í–‰ ìƒíƒœ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.completed_dates = set()

    def save_progress(self):
        """ì§„í–‰ ìƒíƒœ ì €ì¥"""
        try:
            progress_data = {
                'completed_dates': list(self.completed_dates),
                'last_updated': datetime.now().isoformat()
            }
            with open(self.progress_file, 'w', encoding='utf-8') as f:
                json.dump(progress_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            log(f"ì§„í–‰ ìƒíƒœ ì €ì¥ ì‹¤íŒ¨: {e}")

    def get_pending_dates(self):
        """ì•„ì§ ì™„ë£Œë˜ì§€ ì•Šì€ ë‚ ì§œ ëª©ë¡ ë°˜í™˜"""
        return [date for date in self.training_dates if date not in self.completed_dates]

    def collect_parallel_data(self):
        """ë³‘ë ¬ ì²˜ë¦¬ë¡œ 3ë…„ì¹˜ ë°ì´í„° ìˆ˜ì§‘"""
        log("="*60)
        log("ğŸš€ ë³‘ë ¬ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘!")
        log("="*60)
        
        pending_dates = self.get_pending_dates()
        
        if not pending_dates:
            log("ëª¨ë“  ë‚ ì§œ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!")
            return True
        
        log(f"ìˆ˜ì§‘ ëŒ€ìƒ: {len(pending_dates)}ì¼")
        
        try:
            # ProcessPoolExecutorë¥¼ ì‚¬ìš©í•œ ë³‘ë ¬ ì²˜ë¦¬
            with ProcessPoolExecutor(max_workers=self.num_processes) as executor:
                # ê° ë‚ ì§œì— ëŒ€í•œ ì‘ì—… ì œì¶œ
                future_to_date = {
                    executor.submit(process_single_date, date_str, self.grid_points, self.data_dir): date_str
                    for date_str in pending_dates
                }
                
                # ì™„ë£Œëœ ì‘ì—… ì²˜ë¦¬
                completed_count = 0
                for future in as_completed(future_to_date):
                    date_str = future_to_date[future]
                    try:
                        result = future.result()
                        if result:
                            self.completed_dates.add(result)
                            completed_count += 1
                            log(f"âœ… ì§„í–‰: {completed_count}/{len(pending_dates)} ({result})")
                            
                            # ì§„í–‰ ìƒíƒœ ì €ì¥ (10ê°œë§ˆë‹¤)
                            if completed_count % 10 == 0:
                                self.save_progress()
                        else:
                            log(f"âŒ ì‹¤íŒ¨: {date_str}")
                    except Exception as e:
                        log(f"âŒ {date_str} ì˜ˆì™¸ ë°œìƒ: {e}")
                
                # ìµœì¢… ì§„í–‰ ìƒíƒœ ì €ì¥
                self.save_progress()
                
            log(f"ë³‘ë ¬ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {completed_count}/{len(pending_dates)}")
            return True
            
        except Exception as e:
            log(f"ë³‘ë ¬ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return False

    def combine_collected_data(self):
        """ìˆ˜ì§‘ëœ ëª¨ë“  ë‚ ì§œ ë°ì´í„°ë¥¼ í•˜ë‚˜ë¡œ í†µí•©"""
        log("="*60)
        log("ğŸ“Š ë°ì´í„° í†µí•© ì‹œì‘!")
        log("="*60)
        
        try:
            all_dataframes = []
            
            # ëª¨ë“  ë‚ ì§œ íŒŒì¼ ë¡œë“œ
            for date_str in self.training_dates:
                date_file = os.path.join(self.data_dir, f"data_{date_str.replace('-', '_')}.csv")
                
                if os.path.exists(date_file):
                    try:
                        df = pd.read_csv(date_file, encoding='utf-8')
                        if not df.empty:
                            all_dataframes.append(df)
                            log(f"âœ… {date_str}: {len(df)}í–‰")
                        else:
                            log(f"âš ï¸  {date_str}: ë¹ˆ ë°ì´í„°")
                    except Exception as e:
                        log(f"âŒ {date_str} ë¡œë“œ ì‹¤íŒ¨: {e}")
                else:
                    log(f"âš ï¸  {date_str}: íŒŒì¼ ì—†ìŒ")
            
            if not all_dataframes:
                log("í†µí•©í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
                return pd.DataFrame()
            
            # ë°ì´í„° í†µí•©
            combined_df = pd.concat(all_dataframes, ignore_index=True)
            
            # NaN ê°’ ì²˜ë¦¬
            combined_df = combined_df.fillna(0)
            
            log(f"ğŸ“Š ë°ì´í„° í†µí•© ì™„ë£Œ: {len(combined_df)}í–‰ Ã— {len(combined_df.columns)}ì—´")
            log(f"ğŸ“… ë‚ ì§œ ë²”ìœ„: {len(set(combined_df['date']))}ì¼")
            
            # í†µí•© ë°ì´í„° ì €ì¥
            output_file = "parallel_three_year_integrated_data.csv"
            combined_df.to_csv(output_file, index=False, encoding='utf-8')
            log(f"ğŸ’¾ í†µí•© ë°ì´í„° ì €ì¥: {output_file}")
            
            return combined_df
            
        except Exception as e:
            log(f"ë°ì´í„° í†µí•© ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return pd.DataFrame()

    def train_models(self, data_df):
        """AI ëª¨ë¸ í›ˆë ¨"""
        log("="*60)
        log("ğŸ¤– AI ëª¨ë¸ í›ˆë ¨ ì‹œì‘!")
        log("="*60)
        
        try:
            from sklearn.ensemble import RandomForestRegressor
            from sklearn.model_selection import train_test_split
            from sklearn.metrics import r2_score
            import joblib
            
            # íŠ¹ì„± ì»¬ëŸ¼ ì¤€ë¹„
            feature_cols = ['lat', 'lon']
            
            # í™˜ê²½ ë°ì´í„° ì»¬ëŸ¼ ì¶”ê°€
            env_cols = ['sea_water_temperature', 'sea_water_salinity', 'sea_surface_height', 
                       'mixed_layer_depth', 'dissolved_oxygen', 'net_primary_productivity']
            feature_cols.extend([col for col in env_cols if col in data_df.columns])
            
            # ëŒ€ìƒ ì¢… ëª©ë¡
            target_species = self.data_collector.target_species
            
            models = {}
            
            for species in target_species:
                # ì •í™•í•œ ì»¬ëŸ¼ëª… ì‚¬ìš© (GBIF ê´€ì¸¡ ìˆ˜)
                species_col = f"{species.replace(' ', '_')}_gbif_observations"
                
                if species_col in data_df.columns:
                    # ê´€ì¸¡ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸ (0ì´ ì•„ë‹Œ ê°’ì´ ìˆëŠ”ì§€)
                    if data_df[species_col].sum() > 0:
                        log(f"ğŸ¯ {species} ëª¨ë¸ í›ˆë ¨ ì¤‘... (ê´€ì¸¡ìˆ˜: {data_df[species_col].sum()})")
                        
                        # ë°ì´í„° ì¤€ë¹„
                        X = data_df[feature_cols].values
                        y = data_df[species_col].values
                        
                        # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
                        X_train, X_test, y_train, y_test = train_test_split(
                            X, y, test_size=0.2, random_state=42
                        )
                        
                        # ëª¨ë¸ í›ˆë ¨
                        model = RandomForestRegressor(
                            n_estimators=100,
                            max_depth=10,
                            random_state=42,
                            n_jobs=-1
                        )
                        model.fit(X_train, y_train)
                        
                        # ì„±ëŠ¥ í‰ê°€
                        train_score = r2_score(y_train, model.predict(X_train))
                        test_score = r2_score(y_test, model.predict(X_test))
                        
                        models[species] = {
                            'model': model,
                            'features': feature_cols,
                            'train_score': train_score,
                            'test_score': test_score,
                            'target_column': species_col
                        }
                        
                        log(f"âœ… {species}: í›ˆë ¨ RÂ²={train_score:.3f}, í…ŒìŠ¤íŠ¸ RÂ²={test_score:.3f}")
                    else:
                        log(f"âš ï¸ {species}: ê´€ì¸¡ ë°ì´í„° ì—†ìŒ (ëª¨ë“  ê°’ì´ 0)")
                else:
                    log(f"âŒ {species}: ì»¬ëŸ¼ ì—†ìŒ ({species_col})")
            
            self.models = models
            log(f"ğŸ‰ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ: {len(models)}ê°œ ëª¨ë¸")
            return True
            
        except Exception as e:
            log(f"ëª¨ë¸ í›ˆë ¨ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return False

    def save_models(self):
        """í›ˆë ¨ëœ ëª¨ë¸ ì €ì¥"""
        log("="*60)
        log("ğŸ’¾ ëª¨ë¸ ì €ì¥ ì‹œì‘!")
        log("="*60)
        
        try:
            import joblib
            saved_files = []
            
            # Joblib ëª¨ë¸ ì €ì¥
            for target, model_info in self.models.items():
                model_file = f"parallel_three_year_model_{target}.joblib"
                joblib.dump(model_info, model_file)
                saved_files.append(model_file)
                log(f"Joblib ì €ì¥: {model_file}")
            
            # PMML ëª¨ë¸ ì €ì¥ (sklearn2pmml ìˆëŠ” ê²½ìš°)
            try:
                from sklearn2pmml import sklearn2pmml, PMMLPipeline
                from sklearn.preprocessing import StandardScaler
                
                for target, model_info in self.models.items():
                    model = model_info['model']
                    features = model_info['features']
                    
                    # PMML íŒŒì´í”„ë¼ì¸ ìƒì„±
                    pipeline = PMMLPipeline([
                        ("scaler", StandardScaler()),
                        ("regressor", model)
                    ])
                    
                    # ë”ë¯¸ ë°ì´í„°ë¡œ íŒŒì´í”„ë¼ì¸ ë§ì¶¤ (PMML ìƒì„±ìš©)
                    dummy_X = np.random.random((10, len(features)))
                    dummy_y = np.random.random(10)
                    pipeline.fit(dummy_X, dummy_y)
                    
                    # PMML ì €ì¥
                    pmml_path = f"parallel_three_year_model_{target}.pmml"
                    sklearn2pmml(pipeline, pmml_path, with_repr=True)
                    saved_files.append(pmml_path)
                    log(f"PMML ì €ì¥: {pmml_path}")
                    
            except ImportError:
                log("sklearn2pmml íŒ¨í‚¤ì§€ ì—†ìŒ - PMML ì €ì¥ ê±´ë„ˆëœ€")
            except Exception as e:
                log(f"PMML ì €ì¥ ì‹¤íŒ¨: {e}")
            
            log(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {len(saved_files)}ê°œ íŒŒì¼")
            return True
            
        except Exception as e:
            log(f"ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False

    def run_parallel_training(self):
        """ë³‘ë ¬ 3ë…„ì¹˜ ì „ì²´ í›ˆë ¨ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        log("ğŸŒŠ ë³‘ë ¬ 3ë…„ì¹˜ ì‹¤ì œ CMEMS + í•´ì–‘ìƒë¬¼ ë°ì´í„° í†µí•© AI í•™ìŠµ")
        log("="*70)
        
        start_time = time.time()
        
        try:
            # 1. ë³‘ë ¬ ë°ì´í„° ìˆ˜ì§‘
            if not self.collect_parallel_data():
                log("âŒ ë³‘ë ¬ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")
                return False
            
            # 2. ë°ì´í„° í†µí•©
            integrated_df = self.combine_collected_data()
            if integrated_df.empty:
                log("âŒ ë°ì´í„° í†µí•© ì‹¤íŒ¨")
                return False
            
            # 3. ëª¨ë¸ í›ˆë ¨
            if not self.train_models(integrated_df):
                log("âŒ ëª¨ë¸ í›ˆë ¨ ì‹¤íŒ¨")
                return False
            
            # 4. ëª¨ë¸ ì €ì¥
            if not self.save_models():
                log("âŒ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨")
                return False
            
            # 5. ìµœì¢… ê²°ê³¼
            end_time = time.time()
            elapsed_time = end_time - start_time
            
            log("="*70)
            log("ğŸ‰ ë³‘ë ¬ 3ë…„ì¹˜ ì‹¤ì œ CMEMS+ìƒë¬¼ ë°ì´í„° í•™ìŠµ ì™„ë£Œ!")
            log(f"â±ï¸  ì†Œìš” ì‹œê°„: {elapsed_time/60:.1f}ë¶„")
            log(f"âš¡ ë³‘ë ¬ í”„ë¡œì„¸ìŠ¤: {self.num_processes}ê°œ")
            log(f"ğŸ“… ê¸°ê°„: {self.start_date.strftime('%Y-%m-%d')} ~ {self.end_date.strftime('%Y-%m-%d')}")
            log(f"ğŸ“ ê²©ìì : {len(self.grid_points)}ê°œ")
            log(f"ğŸ“Š ìµœì¢… ë°ì´í„°: {len(integrated_df)}í–‰ Ã— {len(integrated_df.columns)}ì—´")
            log(f"ğŸ¤– í›ˆë ¨ëœ ëª¨ë¸: {len(self.models)}ê°œ")
            
            for target, model_info in self.models.items():
                log(f"   â€¢ {target}: í›ˆë ¨ RÂ²={model_info['train_score']:.3f}, í…ŒìŠ¤íŠ¸ RÂ²={model_info['test_score']:.3f}")
            
            log("="*70)
            return True
            
        except Exception as e:
            log(f"âŒ ë³‘ë ¬ í›ˆë ¨ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return False

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ë³‘ë ¬ ì²˜ë¦¬ 3ë…„ì¹˜ ì‹¤ì œ CMEMS + í•´ì–‘ìƒë¬¼ ë°ì´í„° í†µí•© AI í•™ìŠµ")
    print("="*70)
    
    try:
        # CPU ê°œìˆ˜ì— ë”°ë¥¸ í”„ë¡œì„¸ìŠ¤ ìˆ˜ ìë™ ì„¤ì •
        import multiprocessing
        num_cpus = multiprocessing.cpu_count()
        num_processes = min(num_cpus, 6)  # ìµœëŒ€ 6ê°œ í”„ë¡œì„¸ìŠ¤
        
        print(f"ğŸ–¥ï¸  CPU ê°œìˆ˜: {num_cpus}")
        print(f"âš¡ ì‚¬ìš©í•  í”„ë¡œì„¸ìŠ¤ ìˆ˜: {num_processes}")
        print("="*70)
        
        # ë³‘ë ¬ íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™” ë° ì‹¤í–‰
        trainer = ParallelThreeYearTrainer(num_processes=num_processes)
        success = trainer.run_parallel_training()
        
        if success:
            print("âœ… ë³‘ë ¬ í›ˆë ¨ ì„±ê³µ!")
        else:
            print("âŒ ë³‘ë ¬ í›ˆë ¨ ì‹¤íŒ¨!")
            
    except KeyboardInterrupt:
        print("\nâ¹ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
