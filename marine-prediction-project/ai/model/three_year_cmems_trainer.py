#!/usr/bin/env python3
"""
3ë…„ì¹˜ ì‹¤ì œ CMEMS + í•´ì–‘ìƒë¬¼ ë°ì´í„° í†µí•© í•™ìŠµ ì‹œìŠ¤í…œ
- ì‹¤ì œ CMEMS API ì‚¬ìš© (JSON íŒŒì¼ì—ì„œ í™•ì¸í•œ ë³€ìˆ˜ëª…)
- ì‹¤ì œ GBIF/OBIS í•´ì–‘ìƒë¬¼ ê´€ì¸¡ ë°ì´í„° ì‚¬ìš©
- 3ë…„ì¹˜ ë°ì´í„° â†’ í•™ìŠµ â†’ PMML ë‚´ë³´ë‚´ê¸°
- ì¼ì¼ ë°ì´í„° ìë™ ì •ë¦¬ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
"""

import os
import sys
import pandas as pd
import numpy as np
import xarray as xr
from datetime import datetime, timedelta
import logging
import joblib
import threading
import time
import glob
import copernicusmarine
from concurrent.futures import ThreadPoolExecutor, as_completed

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë™ê¸°í™”ë¥¼ ìœ„í•œ ê¸€ë¡œë²Œ ë½
download_locks = {}
lock_manager_lock = threading.Lock()

# ì£¼ìš” ê²½ë¡œ
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.abspath(os.path.join(BASE_DIR, "..", "data"))
CMEMS_DIR = os.path.abspath(os.path.join(BASE_DIR, "..", "..", "cmems_output"))
os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(CMEMS_DIR, exist_ok=True)

LOG_PATH = os.path.join(BASE_DIR, "three_year_training.log")

def log(message):
    """ë¡œê·¸ ë©”ì‹œì§€ ì¶œë ¥"""
    logger.info(message)
    try:
        with open(LOG_PATH, "a", encoding='utf-8') as f:
            f.write(f"{datetime.now()}: {message}\n")
    except:
        pass

# real_cmems_trainer_fixed.pyì—ì„œ ê²€ì¦ëœ í•¨ìˆ˜ë“¤ ë³µì‚¬
def get_dataset_config():
    """CMEMS ë°ì´í„°ì…‹ê³¼ ë³€ìˆ˜ëª… ì„¤ì • (JSON íŒŒì¼ì—ì„œ í™•ì¸í•œ ì‹¤ì œ ë³€ìˆ˜ëª… ì‚¬ìš©)"""
    return {
        "physics": {
            "dataset_id": "cmems_mod_glo_phy_anfc_0.083deg_P1D-m",
            "variables": {
                "temperature": "tob",     # sea_water_potential_temperature_at_sea_floor
                "salinity": "sob",        # sea_water_salinity_at_sea_floor
                "sea_surface_height": "zos",      # sea_surface_height_above_geoid
                "mixed_layer_depth": "mlotst"     # ocean_mixed_layer_thickness_defined_by_sigma_theta
            }
        },
        "biogeochemistry": {
            "dataset_id": "cmems_mod_glo_bgc-bio_anfc_0.25deg_P1D-m", 
            "variables": {
                "dissolved_oxygen": "o2",         # mole_concentration_of_dissolved_molecular_oxygen_in_sea_water
                "net_primary_productivity": "nppv"  # net_primary_production_of_biomass_expressed_as_carbon_per_unit_volume_in_sea_water
            }
        }
    }

def get_file_lock(file_path):
    """íŒŒì¼ë³„ ë½ ë°˜í™˜"""
    global download_locks
    with lock_manager_lock:
        if file_path not in download_locks:
            download_locks[file_path] = threading.Lock()
        return download_locks[file_path]

def cleanup_duplicate_nc_files(date_str):
    """ì¤‘ë³µ ë‹¤ìš´ë¡œë“œëœ .nc íŒŒì¼ë“¤ ì •ë¦¬"""
    try:
        patterns = [
            f"*{date_str}*.nc",
            f"*cmems*{date_str}*.nc"
        ]
        
        for pattern in patterns:
            files = glob.glob(os.path.join(CMEMS_DIR, pattern))
            if len(files) > 2:  # phyì™€ bgc 2ê°œë³´ë‹¤ ë§ìœ¼ë©´ ì¤‘ë³µ
                files.sort(key=os.path.getmtime, reverse=True)
                # ìµœì‹  2ê°œë§Œ ë‚¨ê¸°ê³  ì‚­ì œ
                for old_file in files[2:]:
                    try:
                        os.remove(old_file)
                        log(f"[cleanup] ì¤‘ë³µ íŒŒì¼ ì‚­ì œ: {old_file}")
                    except Exception as e:
                        log(f"[cleanup] ì‚­ì œ ì‹¤íŒ¨: {old_file} - {e}")
                        
    except Exception as e:
        log(f"[cleanup] ì˜¤ë¥˜: {date_str} - {e}")

def cleanup_daily_nc_files(date_str):
    """í•˜ë£¨ì¹˜ NC íŒŒì¼ ì‚­ì œ (ë©”ëª¨ë¦¬ ì ˆì•½)"""
    try:
        patterns = [
            f"cmems_phy_{date_str}.nc",
            f"cmems_bgc_{date_str}.nc"
        ]
        
        for pattern in patterns:
            file_path = os.path.join(CMEMS_DIR, pattern)
            if os.path.exists(file_path):
                os.remove(file_path)
                log(f"[cleanup] ì¼ì¼ íŒŒì¼ ì‚­ì œ: {file_path}")
                
    except Exception as e:
        log(f"[cleanup] ì¼ì¼ ì •ë¦¬ ì‹¤íŒ¨: {date_str} - {e}")

def download_with_lock(nc_path, dataset_id, variables, start_datetime, end_datetime):
    """CMEMS ë°ì´í„° ë‹¤ìš´ë¡œë“œ - íŒŒì¼ ì ê¸ˆìœ¼ë¡œ ì¤‘ë³µ ë‹¤ìš´ë¡œë“œ ë°©ì§€"""
    
    # íŒŒì¼ë³„ ë½ íšë“
    file_lock = get_file_lock(nc_path)
    
    with file_lock:
        # ë½ ë‚´ì—ì„œ ë‹¤ì‹œ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        if os.path.exists(nc_path):
            log(f"[download_with_lock] íŒŒì¼ ì´ë¯¸ ì¡´ì¬: {nc_path}")
            return True
            
        # ì¤‘ë³µ íŒŒì¼ë“¤ì´ ìˆë‹¤ë©´ ë¨¼ì € ì •ë¦¬
        date_str = os.path.basename(nc_path).replace("cmems_phy_", "").replace("cmems_bgc_", "").replace(".nc", "")
        cleanup_duplicate_nc_files(date_str)

        try:
            log(f"[download_with_lock] ë‹¤ìš´ë¡œë“œ ì‹œì‘: {dataset_id}")
            os.makedirs(os.path.dirname(nc_path), exist_ok=True)

            # ì„ì‹œ íŒŒì¼ëª…ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ (CMEMSê°€ ìë™ìœ¼ë¡œ .ncë¥¼ ì¶”ê°€í•¨)
            temp_nc_path = nc_path + ".temp"
            
            copernicusmarine.subset(
                dataset_id=dataset_id,
                variables=variables,
                start_datetime=start_datetime,
                end_datetime=end_datetime,
                minimum_longitude=124.0,
                maximum_longitude=132.0,
                minimum_latitude=33.0,
                maximum_latitude=39.0,
                output_filename=temp_nc_path,
                overwrite=True  # ì„ì‹œ íŒŒì¼ì€ ë®ì–´ì“°ê¸° í—ˆìš©
            )

            # ë‹¤ìš´ë¡œë“œ ì™„ë£Œ í›„ ì›ë˜ ì´ë¦„ìœ¼ë¡œ ì´ë™ (CMEMSê°€ .ncë¥¼ ì¶”ê°€í•˜ë¯€ë¡œ í™•ì¸)
            temp_files = [temp_nc_path, temp_nc_path + ".nc"]
            success_file = None
            
            for temp_file in temp_files:
                if os.path.exists(temp_file):
                    success_file = temp_file
                    break
            
            if success_file:
                os.rename(success_file, nc_path)
                log(f"[download_with_lock] ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {nc_path}")
                return True
            else:
                log(f"[download_with_lock] ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: ì„ì‹œ íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•ŠìŒ")
                log(f"[download_with_lock] í™•ì¸í•œ ê²½ë¡œ: {temp_files}")
                return False

        except Exception as e:
            log(f"[download_with_lock] ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {nc_path} - {e}")
            
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬ (CMEMSê°€ .ncë¥¼ ì¶”ê°€í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë‘ ê°€ì§€ í™•ì¸)
            temp_files = [nc_path + ".temp", nc_path + ".temp.nc"]
            for temp_file in temp_files:
                try:
                    if os.path.exists(temp_file):
                        os.remove(temp_file)
                        log(f"[download_with_lock] ì„ì‹œ íŒŒì¼ ì •ë¦¬: {temp_file}")
                except Exception:
                    pass
                
            return False

def download_cmems_data_for_date(data_type: str, target_date: str):
    """íŠ¹ì • ë‚ ì§œì˜ CMEMS ë°ì´í„° ë‹¤ìš´ë¡œë“œ"""
    try:
        date_obj = datetime.strptime(target_date, '%Y-%m-%d')
        date_str = date_obj.strftime('%Y%m%d')
        
        start_datetime = date_obj.strftime('%Y-%m-%dT00:00:00')
        end_datetime = date_obj.strftime('%Y-%m-%dT23:59:59')
        
        datasets = get_dataset_config()
        
        if data_type == "physics":
            dataset = datasets["physics"]
            nc_path = os.path.join(CMEMS_DIR, f"cmems_phy_{date_str}.nc")
            variables = list(dataset["variables"].values())
        elif data_type == "biogeochemistry":
            dataset = datasets["biogeochemistry"]
            nc_path = os.path.join(CMEMS_DIR, f"cmems_bgc_{date_str}.nc")
            variables = list(dataset["variables"].values())
        else:
            return False
        
        return download_with_lock(
            nc_path=nc_path,
            dataset_id=dataset["dataset_id"],
            variables=variables,
            start_datetime=start_datetime,
            end_datetime=end_datetime
        )
        
    except Exception as e:
        log(f"[DOWNLOAD] {data_type} {target_date} ì‹¤íŒ¨: {e}")
        return False

def collect_cmems_data_for_date(target_date: str, grid_points):
    """íŠ¹ì • ë‚ ì§œì˜ CMEMS ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì—¬ DataFrameìœ¼ë¡œ ë°˜í™˜"""
    try:
        log(f"[CMEMS] {target_date} ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
        
        date_obj = datetime.strptime(target_date, '%Y-%m-%d')
        date_str = date_obj.strftime('%Y%m%d')
        
        # 1. CMEMS ë°ì´í„° ë‹¤ìš´ë¡œë“œ
        log(f"[CMEMS] {target_date} ë¬¼ë¦¬ ë°ì´í„° ë‹¤ìš´ë¡œë“œ...")
        phy_success = download_cmems_data_for_date("physics", target_date)
        
        log(f"[CMEMS] {target_date} ìƒí™”í•™ ë°ì´í„° ë‹¤ìš´ë¡œë“œ...")
        bgc_success = download_cmems_data_for_date("biogeochemistry", target_date)
        
        if not (phy_success and bgc_success):
            log(f"[CMEMS] {target_date} ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ - phy:{phy_success}, bgc:{bgc_success}")
            return pd.DataFrame()
        
        # 2. NetCDF íŒŒì¼ ë¡œë“œ
        phy_nc = os.path.join(CMEMS_DIR, f"cmems_phy_{date_str}.nc")
        bgc_nc = os.path.join(CMEMS_DIR, f"cmems_bgc_{date_str}.nc")
        
        phy_ds = xr.open_dataset(phy_nc)
        bgc_ds = xr.open_dataset(bgc_nc)
        
        # 3. ê²©ìì ë³„ ë°ì´í„° ì¶”ì¶œ
        cmems_data = []
        for i, (lat, lon) in enumerate(grid_points):
            try:
                # ë¬¼ë¦¬ ë°ì´í„° ì¶”ì¶œ
                phy_point = phy_ds.sel(latitude=lat, longitude=lon, method='nearest')
                bgc_point = bgc_ds.sel(latitude=lat, longitude=lon, method='nearest')
                
                row_data = {
                    'lat': lat,
                    'lon': lon
                }
                
                # ë¬¼ë¦¬ ë°ì´í„° ì¶”ì¶œ - ì•ˆì „í•œ ìŠ¤ì¹¼ë¼ ê°’ ì¶”ì¶œ
                def safe_extract_value(data_array, default_val=0.0):
                    """xarray DataArrayì—ì„œ ìŠ¤ì¹¼ë¼ ê°’ì„ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ"""
                    try:
                        if hasattr(data_array, 'values'):
                            val = data_array.values
                            # numpy ìŠ¤ì¹¼ë¼ë‚˜ ë°°ì—´ì„ floatìœ¼ë¡œ ë³€í™˜
                            if hasattr(val, 'item'):  # numpy ìŠ¤ì¹¼ë¼
                                return float(val.item())
                            elif hasattr(val, 'flat'):  # numpy ë°°ì—´
                                return float(next(iter(val.flat)))
                            else:
                                return float(val)
                        else:
                            return float(data_array)
                    except (ValueError, TypeError, IndexError, AttributeError):
                        return default_val
                
                try:
                    # ë¬¼ë¦¬ ë³€ìˆ˜ë“¤ ì¶”ì¶œ
                    if 'tob' in phy_ds.data_vars:
                        row_data['sea_water_temperature'] = safe_extract_value(phy_point.tob)
                    
                    if 'sob' in phy_ds.data_vars:
                        row_data['sea_water_salinity'] = safe_extract_value(phy_point.sob)
                    
                    if 'zos' in phy_ds.data_vars:
                        row_data['sea_surface_height'] = safe_extract_value(phy_point.zos)
                    
                    if 'mlotst' in phy_ds.data_vars:
                        row_data['mixed_layer_depth'] = safe_extract_value(phy_point.mlotst)
                        
                except Exception as e:
                    log(f"[CMEMS] ë¬¼ë¦¬ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨ ({lat}, {lon}): {e}")
                
                # ìƒí™”í•™ ë°ì´í„° ì¶”ì¶œ - ì•ˆì „í•œ ìŠ¤ì¹¼ë¼ ê°’ ì¶”ì¶œ
                try:
                    if 'o2' in bgc_ds.data_vars:
                        row_data['dissolved_oxygen'] = safe_extract_value(bgc_point.o2)
                    
                    if 'nppv' in bgc_ds.data_vars:
                        row_data['net_primary_productivity'] = safe_extract_value(bgc_point.nppv)
                except Exception as e:
                    log(f"[CMEMS] ìƒí™”í•™ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨ ({lat}, {lon}): {e}")
                
                cmems_data.append(row_data)
                
            except Exception as e:
                log(f"[CMEMS] ê²©ì ({lat}, {lon}) ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue
        
        # 4. ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        phy_ds.close()
        bgc_ds.close()
        
        if cmems_data:
            df = pd.DataFrame(cmems_data)
            log(f"[CMEMS] {target_date} ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {len(df)}í–‰ Ã— {len(df.columns)}ì—´")
            return df
        else:
            log(f"[CMEMS] {target_date} ë°ì´í„° ì—†ìŒ")
            return pd.DataFrame()
            
    except Exception as e:
        log(f"[CMEMS] {target_date} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return pd.DataFrame()

class ThreeYearCmemsMarineTrainer:
    """3ë…„ì¹˜ ì‹¤ì œ CMEMS + í•´ì–‘ìƒë¬¼ ë°ì´í„° í†µí•© í•™ìŠµ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        # í•´ì–‘ìƒë¬¼ ë°ì´í„° ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        sys.path.append(os.path.dirname(os.path.abspath(__file__)))
        from real_data_system import MarineRealDataCollector
        self.data_collector = MarineRealDataCollector()
        self.models = {}
        
        # í•œêµ­ ì—°ì•ˆ ì£¼ìš” ê²©ìì  (ì‹¤ì œ í•´ì–‘ ë°ì´í„°ê°€ ìˆëŠ” ìœ„ì¹˜)
        self.grid_points = [
            # ë™í•´
            (37.5, 129.0), (37.0, 129.5), (36.5, 130.0), (36.0, 130.5),
            (35.5, 129.5), (35.0, 129.0), (34.5, 129.5), (34.0, 130.0),
            
            # ë‚¨í•´
            (34.0, 128.5), (34.5, 128.0), (35.0, 127.5), (35.5, 127.0),
            (34.0, 127.0), (34.5, 126.5), (35.0, 126.0), (35.5, 125.5),
            
            # ì„œí•´
            (37.0, 126.0), (36.5, 126.5), (36.0, 127.0), (35.5, 126.0),
            (35.0, 125.5), (34.5, 125.0), (34.0, 124.5), (33.5, 125.0),
        ]
        
        # 3ë…„ì¹˜ ë‚ ì§œ ë²”ìœ„ (CMEMS ë°ì´í„° ê°€ìš© ë²”ìœ„)
        self.start_date = datetime(2022, 6, 1)  # CMEMS ë°ì´í„° ì‹œì‘
        self.end_date = datetime(2024, 9, 13)   # í™•ì‹¤í•œ ê³¼ê±° ë‚ ì§œ
        
        # ë‚ ì§œ ë¦¬ìŠ¤íŠ¸ ìƒì„±
        self.date_list = []
        current_date = self.start_date
        while current_date <= self.end_date:
            self.date_list.append(current_date.strftime('%Y-%m-%d'))
            current_date += timedelta(days=7)  # ì£¼ê°„ ìƒ˜í”Œë§ìœ¼ë¡œ íš¨ìœ¨ì„± í–¥ìƒ
        
        log(f"3ë…„ì¹˜ CMEMS+ìƒë¬¼ ë°ì´í„° í•™ìŠµ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        log(f"ê²©ìì : {len(self.grid_points)}ê°œ")
        log(f"í•™ìŠµ ë‚ ì§œ: {len(self.date_list)}ì¼ ({self.start_date.strftime('%Y-%m-%d')} ~ {self.end_date.strftime('%Y-%m-%d')})")

    def collect_daily_integrated_data(self, target_date: str):
        """íŠ¹ì • ë‚ ì§œì˜ í•´ì–‘ìƒë¬¼ + CMEMS í™˜ê²½ ë°ì´í„° í†µí•© ìˆ˜ì§‘"""
        try:
            # 1. í•´ì–‘ìƒë¬¼ ê´€ì¸¡ ë°ì´í„° ìˆ˜ì§‘
            biological_df = self.data_collector.collect_daily_training_data(target_date, self.grid_points)
            
            if biological_df.empty:
                log(f"[BIO] {target_date} ìƒë¬¼ ë°ì´í„° ì—†ìŒ")
                return pd.DataFrame()
            
            # 2. CMEMS í•´ì–‘í™˜ê²½ ë°ì´í„° ìˆ˜ì§‘
            cmems_df = collect_cmems_data_for_date(target_date, self.grid_points)
            
            if cmems_df.empty:
                log(f"[CMEMS] {target_date} í™˜ê²½ ë°ì´í„° ì—†ìŒ - ìƒë¬¼ ë°ì´í„°ë§Œ ì‚¬ìš©")
                return biological_df
            
            # 3. ë°ì´í„° í†µí•©
            integrated_df = biological_df.merge(
                cmems_df, 
                on=['lat', 'lon'], 
                how='left', 
                suffixes=('_bio', '_env')
            )
            
            # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
            numeric_cols = integrated_df.select_dtypes(include=[np.number]).columns
            integrated_df[numeric_cols] = integrated_df[numeric_cols].fillna(0)
            
            # 4. ì¼ì¼ NC íŒŒì¼ ì •ë¦¬ (ë©”ëª¨ë¦¬ ì ˆì•½)
            date_str = target_date.replace('-', '')
            cleanup_daily_nc_files(date_str)
            
            log(f"[INTEGRATE] {target_date} í†µí•© ì™„ë£Œ: {len(integrated_df)}í–‰ Ã— {len(integrated_df.columns)}ì—´")
            return integrated_df
            
        except Exception as e:
            log(f"[INTEGRATE] {target_date} ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return pd.DataFrame()

    def collect_three_year_data(self):
        """3ë…„ì¹˜ ë°ì´í„° ìˆ˜ì§‘"""
        log("[3YEAR] 3ë…„ì¹˜ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
        
        all_data = []
        successful_days = 0
        
        for i, target_date in enumerate(self.date_list):
            try:
                log(f"[3YEAR] ì§„í–‰: {i+1}/{len(self.date_list)} ({target_date})")
                
                daily_df = self.collect_daily_integrated_data(target_date)
                
                if not daily_df.empty:
                    # ë‚ ì§œ ì •ë³´ ì¶”ê°€
                    daily_df['date'] = target_date
                    all_data.append(daily_df)
                    successful_days += 1
                    log(f"[3YEAR] {target_date} ì„±ê³µ: {len(daily_df)}í–‰ ì¶”ê°€")
                else:
                    log(f"[3YEAR] {target_date} ë°ì´í„° ì—†ìŒ")
                
                # ì§„í–‰ë¥  í‘œì‹œ
                if (i + 1) % 10 == 0:
                    log(f"[3YEAR] ì§„í–‰ë¥ : {i+1}/{len(self.date_list)} ({successful_days}ì¼ ì„±ê³µ)")
                    
            except Exception as e:
                log(f"[3YEAR] {target_date} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue
        
        if all_data:
            final_df = pd.concat(all_data, ignore_index=True)
            log(f"[3YEAR] 3ë…„ì¹˜ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {len(final_df)}í–‰, {successful_days}ì¼ ì„±ê³µ")
            return final_df
        else:
            log("[3YEAR] ìˆ˜ì§‘ëœ ë°ì´í„° ì—†ìŒ")
            return pd.DataFrame()

    def train_models(self, integrated_df):
        """í†µí•© ë°ì´í„°ë¡œ AI ëª¨ë¸ í›ˆë ¨"""
        log("[TRAIN] 3ë…„ì¹˜ AI ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")
        
        try:
            if integrated_df.empty:
                log("í›ˆë ¨ ë°ì´í„° ì—†ìŒ")
                return False
            
            # íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë³€ìˆ˜ ì„ íƒ
            numeric_cols = integrated_df.select_dtypes(include=[np.number]).columns.tolist()
            targets = ['species_diversity_index', 'biomass_estimate', 'bloom_probability']
            features = [col for col in numeric_cols if col not in targets + ['lat', 'lon']]
            
            if len(features) < 5:
                log(f"íŠ¹ì„± ìˆ˜ ë¶€ì¡±: {len(features)}ê°œ (ìµœì†Œ 5ê°œ í•„ìš”)")
                return False
            
            log(f"ì‚¬ìš© íŠ¹ì„±: {len(features)}ê°œ")
            log(f"í›ˆë ¨ ë°ì´í„°: {len(integrated_df)}í–‰")
            
            X = integrated_df[features].fillna(0)
            
            # ê° íƒ€ê²Ÿë³„ ëª¨ë¸ í›ˆë ¨
            from sklearn.ensemble import RandomForestRegressor
            from sklearn.model_selection import cross_val_score, train_test_split
            
            trained_models = {}
            
            for target in targets:
                if target in integrated_df.columns:
                    y = integrated_df[target].fillna(0)
                    
                    # ë°ì´í„° ë¶„í• 
                    X_train, X_test, y_train, y_test = train_test_split(
                        X, y, test_size=0.2, random_state=42
                    )
                    
                    # Random Forest ëª¨ë¸ (3ë…„ì¹˜ ë°ì´í„°ìš© ë” ê°•ë ¥í•œ ì„¤ì •)
                    model = RandomForestRegressor(
                        n_estimators=200,  # íŠ¸ë¦¬ ê°œìˆ˜ ì¦ê°€
                        max_depth=20,      # ê¹Šì´ ì¦ê°€
                        min_samples_split=5,
                        min_samples_leaf=2,
                        max_features='sqrt',
                        random_state=42,
                        n_jobs=-1
                    )
                    
                    # ëª¨ë¸ í›ˆë ¨
                    model.fit(X_train, y_train)
                    
                    # ì„±ëŠ¥ í‰ê°€
                    train_score = model.score(X_train, y_train)
                    test_score = model.score(X_test, y_test)
                    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')
                    
                    trained_models[target] = {
                        'model': model,
                        'features': features,
                        'train_score': train_score,
                        'test_score': test_score,
                        'cv_score': cv_scores.mean(),
                        'cv_std': cv_scores.std()
                    }
                    
                    log(f"{target}: í›ˆë ¨ RÂ² = {train_score:.3f}, í…ŒìŠ¤íŠ¸ RÂ² = {test_score:.3f}, CV = {cv_scores.mean():.3f}Â±{cv_scores.std():.3f}")
            
            self.models = trained_models
            log(f"[TRAIN] 3ë…„ì¹˜ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ: {len(trained_models)}ê°œ")
            return True
            
        except Exception as e:
            log(f"[TRAIN] ëª¨ë¸ í›ˆë ¨ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return False

    def save_models(self):
        """í›ˆë ¨ëœ ëª¨ë¸ì„ joblibì™€ PMMLë¡œ ì €ì¥"""
        log("[SAVE] 3ë…„ì¹˜ ëª¨ë¸ ì €ì¥ ì‹œì‘...")
        
        try:
            if not self.models:
                log("ì €ì¥í•  ëª¨ë¸ ì—†ìŒ")
                return False
            
            saved_files = []
            
            # 1. joblib í˜•ì‹ìœ¼ë¡œ ì €ì¥
            for target, model_info in self.models.items():
                model = model_info['model']
                
                # joblib ì €ì¥
                joblib_path = f"three_year_cmems_marine_model_{target}.joblib"
                joblib.dump(model, joblib_path)
                saved_files.append(joblib_path)
                log(f"joblib ì €ì¥: {joblib_path}")
            
            # 2. PMML í˜•ì‹ìœ¼ë¡œ ì €ì¥ ì‹œë„
            try:
                from sklearn2pmml import sklearn2pmml, PMMLPipeline
                from sklearn.preprocessing import StandardScaler
                
                for target, model_info in self.models.items():
                    model = model_info['model']
                    features = model_info['features']
                    
                    # PMML íŒŒì´í”„ë¼ì¸ ìƒì„±
                    pipeline = PMMLPipeline([
                        ("scaler", StandardScaler()),
                        ("regressor", model)
                    ])
                    
                    # ë”ë¯¸ ë°ì´í„°ë¡œ íŒŒì´í”„ë¼ì¸ ë§ì¶¤ (PMML ìƒì„±ìš©)
                    dummy_X = np.random.random((10, len(features)))
                    dummy_y = np.random.random(10)
                    pipeline.fit(dummy_X, dummy_y)
                    
                    # PMML ì €ì¥
                    pmml_path = f"three_year_cmems_marine_model_{target}.pmml"
                    sklearn2pmml(pipeline, pmml_path, with_repr=True)
                    saved_files.append(pmml_path)
                    log(f"PMML ì €ì¥: {pmml_path}")
                    
            except ImportError:
                log("sklearn2pmml íŒ¨í‚¤ì§€ ì—†ìŒ - PMML ì €ì¥ ê±´ë„ˆëœ€")
            except Exception as e:
                log(f"PMML ì €ì¥ ì‹¤íŒ¨: {e}")
            
            log(f"[SAVE] 3ë…„ì¹˜ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {len(saved_files)}ê°œ íŒŒì¼")
            return True
            
        except Exception as e:
            log(f"[SAVE] ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False

    def run_three_year_training(self):
        """3ë…„ì¹˜ ì „ì²´ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        log("="*60)
        log("ğŸš€ 3ë…„ì¹˜ ì‹¤ì œ CMEMS+ìƒë¬¼ ë°ì´í„° í•™ìŠµ ì‹œì‘!")
        log("="*60)
        
        try:
            # 1. 3ë…„ì¹˜ ë°ì´í„° ìˆ˜ì§‘
            integrated_df = self.collect_three_year_data()
            
            if integrated_df.empty:
                log("3ë…„ì¹˜ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")
                return False
            
            # 2. ë°ì´í„° ì €ì¥
            data_file = "three_year_cmems_integrated_marine_data.csv"
            integrated_df.to_csv(data_file, index=False, encoding='utf-8')
            log(f"3ë…„ì¹˜ í†µí•© ë°ì´í„° ì €ì¥: {data_file}")
            
            # 3. ëª¨ë¸ í›ˆë ¨
            training_success = self.train_models(integrated_df)
            
            if not training_success:
                log("3ë…„ì¹˜ ëª¨ë¸ í›ˆë ¨ ì‹¤íŒ¨")
                return False
            
            # 4. ëª¨ë¸ ì €ì¥
            save_success = self.save_models()
            
            if not save_success:
                log("3ë…„ì¹˜ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨")
                return False
            
            # 5. ìµœì¢… ê²°ê³¼ ìš”ì•½
            log("="*60)
            log("ğŸ‰ 3ë…„ì¹˜ ì‹¤ì œ CMEMS+ìƒë¬¼ ë°ì´í„° í•™ìŠµ ì™„ë£Œ!")
            log(f"ğŸ“… ê¸°ê°„: {self.start_date.strftime('%Y-%m-%d')} ~ {self.end_date.strftime('%Y-%m-%d')}")
            log(f"ğŸ“ ê²©ìì : {len(self.grid_points)}ê°œ")
            log(f"ğŸ“Š ìµœì¢… ë°ì´í„°: {len(integrated_df)}í–‰ Ã— {len(integrated_df.columns)}ì—´")
            log(f"ğŸ¤– í›ˆë ¨ëœ ëª¨ë¸: {len(self.models)}ê°œ")
            log(f"ğŸ’¾ ì €ì¥ íŒŒì¼: {data_file}")
            
            for target, model_info in self.models.items():
                log(f"   â€¢ {target}: í›ˆë ¨ RÂ²={model_info['train_score']:.3f}, í…ŒìŠ¤íŠ¸ RÂ²={model_info['test_score']:.3f}")
            
            log("="*60)
            return True
            
        except Exception as e:
            log(f"3ë…„ì¹˜ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return False

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸŒŠ 3ë…„ì¹˜ ì‹¤ì œ CMEMS + í•´ì–‘ìƒë¬¼ ë°ì´í„° í†µí•© AI í•™ìŠµ")
    print("="*70)
    
    try:
        # íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™” ë° ì‹¤í–‰
        trainer = ThreeYearCmemsMarineTrainer()
        success = trainer.run_three_year_training()
        
        if success:
            print("\nğŸ‰ ì„±ê³µ! 3ë…„ì¹˜ ì‹¤ì œ CMEMS + í•´ì–‘ìƒë¬¼ ë°ì´í„° AI ëª¨ë¸ ì™„ì„±!")
            print("ğŸ“ ìƒì„±ëœ íŒŒì¼:")
            print("   â€¢ three_year_cmems_marine_model_*.joblib (ëª¨ë¸)")
            print("   â€¢ three_year_cmems_marine_model_*.pmml (PMML)")
            print("   â€¢ three_year_cmems_integrated_marine_data.csv (ë°ì´í„°)")
        else:
            print("\nâŒ ì‹¤íŒ¨! ë¬¸ì œë¥¼ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
            
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ì‚¬ìš©ì ì¤‘ë‹¨")
    except Exception as e:
        print(f"\nğŸ’¥ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
