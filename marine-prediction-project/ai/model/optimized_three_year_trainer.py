#!/usr/bin/env python3
"""
ìµœì í™”ëœ 3ë…„ì¹˜ ì‹¤ì œ CMEMS + í•´ì–‘ìƒë¬¼ ë°ì´í„° í†µí•© AI í•™ìŠµ
- ë‹¨ì¼ í†µí•© CSV íŒŒì¼ ì‚¬ìš©
- .nc íŒŒì¼ ì¦‰ì‹œ ì‚­ì œ
- ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í™•ë³´
"""

import os
import sys
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import logging
from concurrent.futures import ProcessPoolExecutor, as_completed
import json
import tempfile
import shutil

# ê²½ë¡œ ì„¤ì •
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))
sys.path.append(current_dir)

from real_data_system import MarineRealDataCollector

class OptimizedThreeYearTrainer:
    """ìµœì í™”ëœ 3ë…„ì¹˜ ë°ì´í„° í†µí•© AI í›ˆë ¨ ì‹œìŠ¤í…œ + ë³‘ë ¬ì²˜ë¦¬"""
    
    def __init__(self):
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('optimized_training.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
        # ë°ì´í„° ìˆ˜ì§‘ê¸° ì´ˆê¸°í™” (ë³‘ë ¬ì²˜ë¦¬ ì§€ì›)
        self.data_collector = MarineRealDataCollector(max_workers=8)
        
        # ê²©ìì  ì„¤ì • (í•œêµ­ ê·¼í•´) - ì§ì ‘ ìƒì„±
        self.grid_points = self.generate_grid_points()
        
        # í†µí•© ë°ì´í„° íŒŒì¼
        self.integrated_file = "three_year_integrated_data.csv"
        self.batch_size = 30  # 30ì¼ì”© ë°°ì¹˜ ì²˜ë¦¬
        
        # ë‚ ì§œ ë²”ìœ„ ì„¤ì • (CMEMS ë°ì´í„° ê°€ìš© ê¸°ê°„)
        start_date = datetime(2022, 6, 1)
        end_date = datetime(2024, 9, 13)
        self.training_dates = []
        current = start_date
        while current <= end_date:
            self.training_dates.append(current.strftime('%Y-%m-%d'))
            current += timedelta(days=7)  # ì£¼ê°„ ê°„ê²©
        
        self.logger.info(f"ğŸ¯ í›ˆë ¨ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        self.logger.info(f"ğŸ“… ë‚ ì§œ ë²”ìœ„: {len(self.training_dates)}ì¼")
        self.logger.info(f"ğŸ—ºï¸  ê²©ìì : {len(self.grid_points)}ê°œ")
        self.logger.info(f"ğŸ“¦ ë°°ì¹˜ í¬ê¸°: {self.batch_size}ì¼")
    
    def generate_grid_points(self):
        """í•œêµ­ ê·¼í•´ ê²©ìì  ìƒì„±"""
        grid_points = []
        
        # ìœ„ë„: 33.5Â°N ~ 37.5Â°N (0.5ë„ ê°„ê²©)
        # ê²½ë„: 124.5Â°E ~ 130.5Â°E (0.5ë„ ê°„ê²©)
        for lat in np.arange(33.5, 38.0, 0.5):
            for lon in np.arange(124.5, 131.0, 0.5):
                grid_points.append((lat, lon))
        
        return grid_points
    
    def collect_batch_data(self, date_batch):
        """ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë°ì´í„° ë³‘ë ¬ ìˆ˜ì§‘"""
        batch_data = []
        
        # ë³‘ë ¬ë¡œ ì—¬ëŸ¬ ë‚ ì§œ ë™ì‹œ ì²˜ë¦¬
        self.logger.info(f"ï¿½ ë°°ì¹˜ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘: {len(date_batch)}ì¼")
        all_data = self.data_collector.collect_multiple_days_parallel(date_batch, self.grid_points)
        
        if all_data:
            self.logger.info(f"âœ… ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {len(all_data)}ê°œ ë°ì´í„° í¬ì¸íŠ¸")
            return all_data
        else:
            self.logger.warning(f"âš ï¸ ë°°ì¹˜ ë°ì´í„° ì—†ìŒ")
            return []
    
    def cleanup_nc_files(self, date_str):
        """íŠ¹ì • ë‚ ì§œì˜ .nc íŒŒì¼ë“¤ ì‚­ì œ"""
        try:
            nc_files = [
                f"cmems_phy_{date_str.replace('-', '')}.nc",
                f"cmems_bgc_{date_str.replace('-', '')}.nc"
            ]
            
            cmems_dir = os.path.join(os.path.dirname(os.path.dirname(current_dir)), 'cmems_output')
            
            for nc_file in nc_files:
                nc_path = os.path.join(cmems_dir, nc_file)
                if os.path.exists(nc_path):
                    os.remove(nc_path)
                    self.logger.debug(f"ğŸ—‘ï¸ ì‚­ì œ: {nc_file}")
        
        except Exception as e:
            self.logger.warning(f"âš ï¸ .nc íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
    
    def append_to_integrated_file(self, batch_data):
        """ë°°ì¹˜ ë°ì´í„°ë¥¼ í†µí•© íŒŒì¼ì— ì¶”ê°€"""
        if not batch_data:
            return
        
        try:
            # ë°°ì¹˜ ë°ì´í„° í†µí•©
            batch_df = pd.concat(batch_data, ignore_index=True)
            batch_df = batch_df.fillna(0)
            
            # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            file_exists = os.path.exists(self.integrated_file)
            
            # ì²« ë²ˆì§¸ ë°°ì¹˜ë©´ í—¤ë” í¬í•¨, ì•„ë‹ˆë©´ í—¤ë” ì œì™¸í•˜ê³  ì¶”ê°€
            batch_df.to_csv(
                self.integrated_file, 
                mode='a' if file_exists else 'w',
                header=not file_exists,
                index=False, 
                encoding='utf-8'
            )
            
            self.logger.info(f"ğŸ’¾ ë°°ì¹˜ ì €ì¥: {len(batch_df)}í–‰ ì¶”ê°€")
            
        except Exception as e:
            self.logger.error(f"âŒ ë°°ì¹˜ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def collect_all_data(self):
        """ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ (ë°°ì¹˜ ì²˜ë¦¬)"""
        self.logger.info("="*60)
        self.logger.info("ğŸŒŠ ìµœì í™”ëœ 3ë…„ì¹˜ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘!")
        self.logger.info("="*60)
        
        # ê¸°ì¡´ í†µí•© íŒŒì¼ ì‚­ì œ
        if os.path.exists(self.integrated_file):
            os.remove(self.integrated_file)
            self.logger.info("ğŸ—‘ï¸ ê¸°ì¡´ í†µí•© íŒŒì¼ ì‚­ì œ")
        
        total_batches = (len(self.training_dates) + self.batch_size - 1) // self.batch_size
        
        for i in range(0, len(self.training_dates), self.batch_size):
            batch_num = i // self.batch_size + 1
            date_batch = self.training_dates[i:i + self.batch_size]
            
            self.logger.info(f"ğŸ“¦ ë°°ì¹˜ {batch_num}/{total_batches}: {len(date_batch)}ì¼ ì²˜ë¦¬")
            
            # ë°°ì¹˜ ë°ì´í„° ìˆ˜ì§‘
            batch_data = self.collect_batch_data(date_batch)
            
            # í†µí•© íŒŒì¼ì— ì¶”ê°€
            self.append_to_integrated_file(batch_data)
            
            self.logger.info(f"âœ… ë°°ì¹˜ {batch_num} ì™„ë£Œ")
        
        # ìµœì¢… í†µí•© íŒŒì¼ í™•ì¸
        if os.path.exists(self.integrated_file):
            final_df = pd.read_csv(self.integrated_file)
            self.logger.info(f"ğŸ‰ ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!")
            self.logger.info(f"ğŸ“Š ìµœì¢… ë°ì´í„°: {len(final_df)}í–‰ Ã— {len(final_df.columns)}ì—´")
            return final_df
        else:
            self.logger.error("âŒ í†µí•© íŒŒì¼ ìƒì„± ì‹¤íŒ¨")
            return pd.DataFrame()
    
    def train_models(self, data_df):
        """AI ëª¨ë¸ í›ˆë ¨"""
        self.logger.info("="*60)
        self.logger.info("ğŸ¤– AI ëª¨ë¸ í›ˆë ¨ ì‹œì‘!")
        self.logger.info("="*60)
        
        try:
            from sklearn.ensemble import RandomForestRegressor
            from sklearn.model_selection import train_test_split
            from sklearn.metrics import r2_score
            import joblib
            
            # íŠ¹ì„± ì»¬ëŸ¼ ì¤€ë¹„
            feature_cols = ['lat', 'lon']
            
            # í™˜ê²½ ë°ì´í„° ì»¬ëŸ¼ ì¶”ê°€
            env_cols = ['sea_water_temperature', 'sea_water_salinity', 'sea_surface_height', 
                       'mixed_layer_depth', 'dissolved_oxygen', 'net_primary_productivity']
            feature_cols.extend([col for col in env_cols if col in data_df.columns])
            
            # ëŒ€ìƒ ì¢… ëª©ë¡
            target_species = self.data_collector.target_species
            
            models = {}
            models_dir = "optimized_models"
            os.makedirs(models_dir, exist_ok=True)
            
            for species in target_species:
                # ì •í™•í•œ ì»¬ëŸ¼ëª… ì‚¬ìš© (GBIF ê´€ì¸¡ ìˆ˜)
                species_col = f"{species.replace(' ', '_')}_gbif_observations"
                
                if species_col in data_df.columns:
                    # ê´€ì¸¡ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸ (0ì´ ì•„ë‹Œ ê°’ì´ ìˆëŠ”ì§€)
                    if data_df[species_col].sum() > 0:
                        self.logger.info(f"ğŸ¯ {species} ëª¨ë¸ í›ˆë ¨ ì¤‘... (ê´€ì¸¡ìˆ˜: {data_df[species_col].sum()})")
                        
                        # ë°ì´í„° ì¤€ë¹„
                        X = data_df[feature_cols].values
                        y = data_df[species_col].values
                        
                        # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
                        X_train, X_test, y_train, y_test = train_test_split(
                            X, y, test_size=0.2, random_state=42
                        )
                        
                        # ëª¨ë¸ í›ˆë ¨
                        model = RandomForestRegressor(
                            n_estimators=100,
                            max_depth=10,
                            random_state=42,
                            n_jobs=-1
                        )
                        model.fit(X_train, y_train)
                        
                        # ì„±ëŠ¥ í‰ê°€
                        train_score = r2_score(y_train, model.predict(X_train))
                        test_score = r2_score(y_test, model.predict(X_test))
                        
                        # ëª¨ë¸ ì €ì¥
                        model_file = os.path.join(models_dir, f"{species.replace(' ', '_')}_model.joblib")
                        joblib.dump(model, model_file)
                        
                        # ëª¨ë¸ ì •ë³´ ì €ì¥
                        info_file = os.path.join(models_dir, f"{species.replace(' ', '_')}_info.json")
                        model_info = {
                            'species': species,
                            'features': feature_cols,
                            'train_score': train_score,
                            'test_score': test_score,
                            'target_column': species_col,
                            'training_date': datetime.now().isoformat()
                        }
                        
                        with open(info_file, 'w', encoding='utf-8') as f:
                            json.dump(model_info, f, ensure_ascii=False, indent=2)
                        
                        models[species] = model_info
                        
                        self.logger.info(f"âœ… {species}: í›ˆë ¨ RÂ²={train_score:.3f}, í…ŒìŠ¤íŠ¸ RÂ²={test_score:.3f}")
                    else:
                        self.logger.warning(f"âš ï¸ {species}: ê´€ì¸¡ ë°ì´í„° ì—†ìŒ (ëª¨ë“  ê°’ì´ 0)")
                else:
                    self.logger.warning(f"âŒ {species}: ì»¬ëŸ¼ ì—†ìŒ ({species_col})")
            
            self.logger.info(f"ğŸ‰ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ: {len(models)}ê°œ ëª¨ë¸")
            return models
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ í›ˆë ¨ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return {}
    
    def export_to_pmml(self, models):
        """ëª¨ë¸ì„ PMML í˜•ì‹ìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
        self.logger.info("="*60)
        self.logger.info("ğŸ“¤ PMML ëª¨ë¸ ë‚´ë³´ë‚´ê¸°!")
        self.logger.info("="*60)
        
        try:
            from sklearn2pmml import PMMLPipeline, sklearn2pmml
            from sklearn.preprocessing import StandardScaler
            
            pmml_dir = "optimized_pmml_models"
            os.makedirs(pmml_dir, exist_ok=True)
            
            # í†µí•© ë°ì´í„° ë¡œë“œ
            data_df = pd.read_csv(self.integrated_file)
            
            # íŠ¹ì„± ì»¬ëŸ¼ ì¤€ë¹„
            feature_cols = ['lat', 'lon']
            env_cols = ['sea_water_temperature', 'sea_water_salinity', 'sea_surface_height', 
                       'mixed_layer_depth', 'dissolved_oxygen', 'net_primary_productivity']
            feature_cols.extend([col for col in env_cols if col in data_df.columns])
            
            for species, model_info in models.items():
                try:
                    # ëª¨ë¸ ë¡œë“œ
                    model_file = os.path.join("optimized_models", f"{species.replace(' ', '_')}_model.joblib")
                    if os.path.exists(model_file):
                        import joblib
                        model = joblib.load(model_file)
                        
                        # PMML íŒŒì´í”„ë¼ì¸ ìƒì„± (í”¼ì³ ì´ë¦„ í¬í•¨)
                        pipeline = PMMLPipeline([
                            ("scaler", StandardScaler()),
                            ("regressor", model)
                        ])
                        
                        # ë°ì´í„° ì¤€ë¹„ (DataFrame í˜•íƒœë¡œ í”¼ì³ ì´ë¦„ ë³´ì¡´)
                        species_col = model_info['target_column']
                        X_df = data_df[feature_cols].copy()
                        y_series = data_df[species_col].copy()
                        y_series.name = species_col  # íƒ€ê²Ÿ í•„ë“œ ì´ë¦„ ëª…ì‹œ
                        
                        # íŒŒì´í”„ë¼ì¸ í›ˆë ¨ (DataFrameê³¼ Series ì‚¬ìš©)
                        pipeline.fit(X_df, y_series)
                        
                        # PMML ë‚´ë³´ë‚´ê¸°
                        pmml_file = os.path.join(pmml_dir, f"{species.replace(' ', '_')}_model.pmml")
                        sklearn2pmml(pipeline, pmml_file, with_repr=True)
                        
                        self.logger.info(f"âœ… {species} PMML ë‚´ë³´ë‚´ê¸° ì™„ë£Œ")
                
                except Exception as e:
                    self.logger.error(f"âŒ {species} PMML ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
            
            self.logger.info("ğŸ‰ PMML ë‚´ë³´ë‚´ê¸° ì™„ë£Œ!")
            
        except ImportError:
            self.logger.warning("âš ï¸ sklearn2pmmlì´ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ. pip install sklearn2pmmlë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")
        except Exception as e:
            self.logger.error(f"âŒ PMML ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
    
    def run_full_training(self):
        """ì „ì²´ í›ˆë ¨ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        try:
            self.logger.info("ğŸš€ ìµœì í™”ëœ 3ë…„ì¹˜ ì‹¤ì œ CMEMS + í•´ì–‘ìƒë¬¼ ë°ì´í„° í†µí•© AI í•™ìŠµ")
            self.logger.info("="*70)
            
            # 1. ë°ì´í„° ìˆ˜ì§‘
            data_df = self.collect_all_data()
            
            if data_df.empty:
                self.logger.error("âŒ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")
                return False
            
            # 2. ëª¨ë¸ í›ˆë ¨
            models = self.train_models(data_df)
            
            if not models:
                self.logger.error("âŒ ëª¨ë¸ í›ˆë ¨ ì‹¤íŒ¨")
                return False
            
            # 3. PMML ë‚´ë³´ë‚´ê¸°
            self.export_to_pmml(models)
            
            # 4. ìµœì¢… ì •ë¦¬
            self.cleanup_all_nc_files()
            
            self.logger.info("ğŸ‰ ì „ì²´ í›ˆë ¨ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ!")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def cleanup_all_nc_files(self):
        """ëª¨ë“  .nc íŒŒì¼ ì •ë¦¬"""
        try:
            cmems_dir = os.path.join(os.path.dirname(os.path.dirname(current_dir)), 'cmems_output')
            if os.path.exists(cmems_dir):
                nc_files = [f for f in os.listdir(cmems_dir) if f.endswith('.nc')]
                for nc_file in nc_files:
                    os.remove(os.path.join(cmems_dir, nc_file))
                
                self.logger.info(f"ğŸ—‘ï¸ {len(nc_files)}ê°œ .nc íŒŒì¼ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            self.logger.warning(f"âš ï¸ .nc íŒŒì¼ ì •ë¦¬ ì‹¤íŒ¨: {e}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    trainer = OptimizedThreeYearTrainer()
    success = trainer.run_full_training()
    
    if success:
        print("\nğŸ‰ ìµœì í™”ëœ 3ë…„ì¹˜ AI ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!")
        print("ğŸ“ ìƒì„±ëœ íŒŒì¼:")
        print("  - three_year_integrated_data.csv (í†µí•© ë°ì´í„°)")
        print("  - optimized_models/ (joblib ëª¨ë¸)")
        print("  - optimized_pmml_models/ (PMML ëª¨ë¸)")
        print("  - optimized_training.log (ë¡œê·¸)")
    else:
        print("\nâŒ í›ˆë ¨ ì‹¤íŒ¨")

if __name__ == "__main__":
    main()
