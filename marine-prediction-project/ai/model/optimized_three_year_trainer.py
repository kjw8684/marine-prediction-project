#!/usr/bin/env python3
"""
ìµœì í™”ëœ 3ë…„ì¹˜ ì‹¤ì œ CMEMS + í•´ì–‘ìƒë¬¼ ë°ì´í„° í†µí•© AI í•™ìŠµ
- 7ì¼ ê°„ê²© í•™ìŠµìœ¼ë¡œ íš¨ìœ¨ì„± ê°œì„ 
- .nc íŒŒì¼ ì¦‰ì‹œ ì‚­ì œ
- ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í™•ë³´
"""

import os
import sys
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import logging
from concurrent.futures import ProcessPoolExecutor, as_completed
import json
import tempfile
import shutil

# ê²½ë¡œ ì„¤ì •
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))
sys.path.append(current_dir)

from real_data_system import MarineRealDataCollector

class OptimizedThreeYearTrainer:
    """ìµœì í™”ëœ 3ë…„ì¹˜ ë°ì´í„° í†µí•© AI í›ˆë ¨ ì‹œìŠ¤í…œ (7ì¼ ê°„ê²©)"""
    
    def __init__(self):
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('optimized_training.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
        # ë°ì´í„° ìˆ˜ì§‘ê¸° ì´ˆê¸°í™” (ì ì ˆí•œ ë³‘ë ¬ì²˜ë¦¬)
        self.data_collector = MarineRealDataCollector(max_workers=2)
        
        # í†µí•© ë°ì´í„° íŒŒì¼
        self.integrated_file = "three_year_weekly_integrated_data.csv"
        
        # ë‚ ì§œ ë²”ìœ„ ì„¤ì • (CMEMS ë°ì´í„° ê°€ìš© ê¸°ê°„) - 7ì¼ ê°„ê²©
        start_date = datetime(2022, 6, 1)
        end_date = datetime(2024, 9, 13)
        
        self.logger.info(f"ğŸ¯ 7ì¼ ê°„ê²© í›ˆë ¨ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        self.logger.info(f"ğŸ“… ë‚ ì§œ ë²”ìœ„: {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}")
        self.logger.info(f"ğŸ—ºï¸  ê²©ìì : í•œêµ­ê·¼í•´ 0.5ë„ í•´ìƒë„")
        self.logger.info(f"ğŸ“ˆ í•™ìŠµ ë°©ì‹: 7ì¼ ê°„ê²©, ìƒë¬¼ë°ì´í„°ëŠ” Â±3ì¼ ë²”ìœ„")
    
    def collect_weekly_training_data(self, start_date_str, end_date_str):
        """7ì¼ ê°„ê²© í›ˆë ¨ ë°ì´í„° ìˆ˜ì§‘"""
        
        self.logger.info(f"ğŸ”„ 7ì¼ ê°„ê²© ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘: {start_date_str} ~ {end_date_str}")
        
        # ìƒˆë¡œìš´ ì£¼ê°„ ë°ì´í„° ìˆ˜ì§‘ ë©”ì„œë“œ ì‚¬ìš©
        collected_data_path = self.data_collector.collect_weekly_training_data(
            start_date=start_date_str,
            end_date=end_date_str,
            lat_range=(33.5, 37.5),
            lon_range=(124.5, 130.5),
            resolution=0.5
        )
        
        if collected_data_path and os.path.exists(collected_data_path):
            # CSV íŒŒì¼ ì½ê¸°
            df = pd.read_csv(collected_data_path, encoding='utf-8-sig')
            self.logger.info(f"âœ… 7ì¼ ê°„ê²© ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {len(df)}ê°œ ë°ì´í„° í¬ì¸íŠ¸")
            
            return df, collected_data_path
        else:
            self.logger.warning(f"âš ï¸ 7ì¼ ê°„ê²© ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")
            return None, None

    def run_full_training_pipeline(self):
        """ì „ì²´ í›ˆë ¨ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (7ì¼ ê°„ê²©)"""
        
        try:
            self.logger.info("ğŸš€ 3ë…„ì¹˜ 7ì¼ ê°„ê²© í•´ì–‘ AI í›ˆë ¨ ì‹œì‘")
            
            # 1. 3ë…„ì¹˜ ë°ì´í„°ë¥¼ 6ê°œì›”ì”© ë‚˜ëˆ„ì–´ ì²˜ë¦¬
            start_date = datetime(2022, 6, 1)
            end_date = datetime(2024, 9, 13)
            
            all_data = []
            current_start = start_date
            batch_count = 0
            
            while current_start < end_date:
                # 6ê°œì›” ë°°ì¹˜ ì„¤ì •
                current_end = min(current_start + timedelta(days=180), end_date)
                batch_count += 1
                
                self.logger.info(f"ğŸ“¦ ë°°ì¹˜ {batch_count}: {current_start.strftime('%Y-%m-%d')} ~ {current_end.strftime('%Y-%m-%d')}")
                
                # ë°°ì¹˜ ë°ì´í„° ìˆ˜ì§‘
                batch_df, temp_file = self.collect_weekly_training_data(
                    current_start.strftime('%Y-%m-%d'),
                    current_end.strftime('%Y-%m-%d')
                )
                
                if batch_df is not None:
                    all_data.append(batch_df)
                    self.logger.info(f"  âœ… ë°°ì¹˜ {batch_count} ì™„ë£Œ: {len(batch_df)} í–‰")
                    
                    # ì„ì‹œ íŒŒì¼ ì •ë¦¬
                    if temp_file and os.path.exists(temp_file):
                        try:
                            os.remove(temp_file)
                        except:
                            pass
                else:
                    self.logger.warning(f"  âš ï¸ ë°°ì¹˜ {batch_count} ì‹¤íŒ¨")
                
                # ë‹¤ìŒ ë°°ì¹˜ë¡œ
                current_start = current_end + timedelta(days=1)
            
            # 2. ëª¨ë“  ë°ì´í„° í†µí•©
            if all_data:
                self.logger.info("ğŸ”§ ë°ì´í„° í†µí•© ì¤‘...")
                integrated_df = pd.concat(all_data, ignore_index=True)
                
                # í†µí•© íŒŒì¼ ì €ì¥
                integrated_df.to_csv(self.integrated_file, index=False, encoding='utf-8-sig')
                
                self.logger.info(f"ğŸ’¾ í†µí•© ë°ì´í„° ì €ì¥ ì™„ë£Œ: {self.integrated_file}")
                self.logger.info(f"   ì´ ë°ì´í„°: {len(integrated_df):,} í–‰")
                self.logger.info(f"   íŒŒì¼ í¬ê¸°: {os.path.getsize(self.integrated_file) / 1024 / 1024:.1f} MB")
                
                # 3. ë°ì´í„° í’ˆì§ˆ ê²€ì¦
                self.validate_integrated_data(integrated_df)
                
                # 4. AI ëª¨ë¸ í›ˆë ¨
                self.train_ai_models(integrated_df)
                
                return True
            else:
                self.logger.error("âŒ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ í›ˆë ¨ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {e}")
            return False
    
    def validate_integrated_data(self, df):
        """í†µí•© ë°ì´í„° í’ˆì§ˆ ê²€ì¦"""
        
        self.logger.info("ğŸ” ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ì¤‘...")
        
        # ê¸°ë³¸ í†µê³„
        total_rows = len(df)
        total_cols = len(df.columns)
        
        # NULL ê°’ í™•ì¸
        null_counts = df.isnull().sum()
        high_null_cols = null_counts[null_counts > total_rows * 0.5]
        
        # ìƒë¬¼ ë°ì´í„° í™•ì¸
        bio_cols = [col for col in df.columns if '_density' in col]
        env_cols = [col for col in df.columns if col.startswith('cmems_')]
        
        self.logger.info(f"  ğŸ“Š ê¸°ë³¸ ì •ë³´: {total_rows:,} í–‰, {total_cols} ì—´")
        self.logger.info(f"  ğŸŸ ìƒë¬¼ ë³€ìˆ˜: {len(bio_cols)}ê°œ")
        self.logger.info(f"  ğŸŒŠ í™˜ê²½ ë³€ìˆ˜: {len(env_cols)}ê°œ")
        self.logger.info(f"  âš ï¸  ë†’ì€ NULL ì»¬ëŸ¼: {len(high_null_cols)}ê°œ")
        
        if len(high_null_cols) > 0:
            self.logger.warning(f"     NULL ë¹„ìœ¨ ë†’ìŒ: {list(high_null_cols.index)}")
        
        # ë‚ ì§œ ë¶„í¬ í™•ì¸
        if 'collection_date' in df.columns:
            date_counts = df['collection_date'].value_counts()
            self.logger.info(f"  ğŸ“… ìˆ˜ì§‘ ë‚ ì§œ: {len(date_counts)}ê°œ (7ì¼ ê°„ê²©)")
        
        return True
    
    def train_ai_models(self, df):
        """AI ëª¨ë¸ í›ˆë ¨ (ëœë¤ í¬ë ˆìŠ¤íŠ¸ + PMML ë‚´ë³´ë‚´ê¸°)"""
        
        try:
            self.logger.info("ğŸ§  AI ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")
            
            from sklearn.ensemble import RandomForestRegressor
            from sklearn.model_selection import train_test_split
            from sklearn.metrics import mean_squared_error, r2_score
            from sklearn2pmml import sklearn2pmml, PMMLPipeline
            from sklearn2pmml.preprocessing import PMMLLabelEncoder
            from sklearn.preprocessing import StandardScaler
            import joblib
            
            # íŠ¹ì„± ë° íƒ€ê²Ÿ ì¤€ë¹„
            feature_cols = [col for col in df.columns if col.startswith('cmems_') or 
                           col in ['latitude', 'longitude', 'depth_m', 'distance_to_coast_km']]
            
            target_species = [
                'Aurelia_aurita', 'Chrysaora_pacifica', 'Scomber_japonicus',
                'Engraulis_japonicus', 'Todarodes_pacificus', 'Trachurus_japonicus',
                'Sardinops_melanostictus', 'Chaetodon_nippon'
            ]
            
            # ê° ì¢…ë³„ë¡œ ëª¨ë¸ í›ˆë ¨
            for species in target_species:
                density_col = f"{species}_density"
                weight_col = f"{species}_weight"
                
                if density_col not in df.columns:
                    continue
                
                self.logger.info(f"  ğŸŸ {species} ëª¨ë¸ í›ˆë ¨ ì¤‘...")
                
                # ë°ì´í„° ì¤€ë¹„
                valid_data = df.dropna(subset=feature_cols + [density_col])
                
                if len(valid_data) < 100:
                    self.logger.warning(f"    âš ï¸ {species}: ë°ì´í„° ë¶€ì¡± ({len(valid_data)}í–‰)")
                    continue
                
                X = valid_data[feature_cols]
                y = valid_data[density_col]
                
                # ê°€ì¤‘ì¹˜ ì ìš© (ì‹¤ì œ ê´€ì¸¡ ë°ì´í„°ëŠ” ë†’ì€ ê°€ì¤‘ì¹˜)
                sample_weights = valid_data[weight_col] if weight_col in valid_data.columns else None
                
                # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
                X_train, X_test, y_train, y_test = train_test_split(
                    X, y, test_size=0.2, random_state=42
                )
                
                if sample_weights is not None:
                    weights_train = sample_weights.loc[X_train.index]
                    weights_test = sample_weights.loc[X_test.index]
                else:
                    weights_train = None
                    weights_test = None
                
                # ëª¨ë¸ í›ˆë ¨ (ê°€ì¤‘ì¹˜ ì ìš©)
                model = RandomForestRegressor(
                    n_estimators=100,
                    max_depth=15,
                    random_state=42,
                    n_jobs=-1
                )
                
                model.fit(X_train, y_train, sample_weight=weights_train)
                
                # ì˜ˆì¸¡ ë° í‰ê°€
                y_pred = model.predict(X_test)
                mse = mean_squared_error(y_test, y_pred, sample_weight=weights_test)
                r2 = r2_score(y_test, y_pred, sample_weight=weights_test)
                
                self.logger.info(f"    âœ… {species}: MSE={mse:.4f}, RÂ²={r2:.4f}")
                
                # ëª¨ë¸ ì €ì¥ (joblib)
                joblib_path = f"marine_ai_model_{species.lower()}.joblib"
                joblib.dump(model, joblib_path)
                
                # PMML ë‚´ë³´ë‚´ê¸°
                try:
                    pmml_pipeline = PMMLPipeline([
                        ("regressor", model)
                    ])
                    pmml_pipeline.fit(X_train, y_train)
                    
                    pmml_path = f"marine_ai_model_{species.lower()}.pmml"
                    sklearn2pmml(pmml_pipeline, pmml_path)
                    
                    self.logger.info(f"    ğŸ’¾ PMML ì €ì¥: {pmml_path}")
                    
                except Exception as e:
                    self.logger.warning(f"    âš ï¸ PMML ì €ì¥ ì‹¤íŒ¨ ({species}): {e}")
            
            self.logger.info("ğŸ‰ AI ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ AI ëª¨ë¸ í›ˆë ¨ ì‹¤íŒ¨: {e}")
            return False

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    trainer = OptimizedThreeYearTrainer()
    
    try:
        success = trainer.run_full_training_pipeline()
        
        if success:
            print("\nğŸ‰ 3ë…„ì¹˜ 7ì¼ ê°„ê²© í•´ì–‘ AI í›ˆë ¨ ì„±ê³µ!")
            print(f"ğŸ“ í†µí•© ë°ì´í„°: {trainer.integrated_file}")
            print("ğŸ“„ PMML íŒŒì¼ë“¤ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            print("\nâŒ í›ˆë ¨ ì‹¤íŒ¨")
            
    except KeyboardInterrupt:
        print("\nâš ï¸ ì‚¬ìš©ì ì¤‘ë‹¨")
    except Exception as e:
        print(f"\nâŒ ì‹¤í–‰ ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    main()
